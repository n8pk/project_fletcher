{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import os\n",
    "\n",
    "chromedriver = \"/home/nate/Bench/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrape leafly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting to scrape, always have to check the layout of the site and build a game plan.\n",
    "\n",
    "For leafly, there's a page listing all of the strains of marijuana that have been reviewed, ordered by\n",
    "\n",
    "number of reviews. It start's off just showing the top ~50. For the MVP I used beautiful soup to grab\n",
    "\n",
    "these links, and later extended that code with a selenium script that clicked 'show me more' several\n",
    "\n",
    "times. It's not more than a loop of clicks, with a url request, so I haven't included it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = pd.read_pickle('link_list.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium scripts look way more intimidating than they really are. Basically just\n",
    "\n",
    "enter developer mode 'CTRL+SHIFT+C', click on the object you're interested, and\n",
    "\n",
    "copy x-path, boom success. For finicky sites (and internet connections) I prefer\n",
    "\n",
    "to use try/except statements with sleep timers inside my functions, that way if\n",
    "\n",
    "something goes wrong, it's easier to keep going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strain_scrape(strain_link, num_pages):\n",
    "    \n",
    "    '''\n",
    "    given a link, scrape the reviews from each page for the given number of pages\n",
    "    '''\n",
    "    \n",
    "    reviews = []\n",
    "    \n",
    "    for page in range(num_pages):\n",
    "        time.sleep(5)\n",
    "        driver.get('https://www.leafly.com' + strain_link + '/reviews?page={}&sort=date'.format(page))\n",
    "        \n",
    "        for review in range(1,9):\n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                driver.find_element_by_xpath('//*[@id=\"main\"]/div/section/div[2]/div[2]/ul/li[{}]/div/div[2]/div[2]/div[1]/div/a'.format(review)).click()\n",
    "                review_text = driver.find_element_by_xpath('//*[@id=\"main\"]/div/section/div[2]/div[2]/div[4]/div[1]').text\n",
    "                reviews.append(review_text)\n",
    "                driver.execute_script(\"window.history.go(-1)\")\n",
    "            except:\n",
    "                try:\n",
    "                    time.sleep(5)\n",
    "                    driver.navigate.refresh()\n",
    "                    driver.find_element_by_xpath('//*[@id=\"main\"]/div/section/div[2]/div[2]/ul/li[{}]/div/div[2]/div[2]/div[1]/div/a'.format(review)).click()\n",
    "                    review_text = driver.find_element_by_xpath('//*[@id=\"main\"]/div/section/div[2]/div[2]/div[4]/div[1]').text\n",
    "                    reviews.append(review_text)\n",
    "                    driver.execute_script(\"window.history.go(-1)\")\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leafly has an initial popup that has one of two paths, and\n",
    "\n",
    "then a second popup asking for an email address!\n",
    "\n",
    "This next cell deals with that for us, and sets up the page for our scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(chromedriver)\n",
    "driver.get('https://www.leafly.com')\n",
    "try:\n",
    "    driver.find_element_by_xpath('/html/body/div[6]/div/div/div/form/div[1]/div[1]/fieldset/label').click()\n",
    "    driver.find_element_by_xpath('/html/body/div[6]/div/div/div/form/div[2]/button').click()\n",
    "except:\n",
    "    driver.find_element_by_xpath('/html/body/div[6]/div/div/form/fieldset/div/span[1]/label').click()\n",
    "    driver.find_element_by_xpath('/html/body/div[6]/div/div/form/div/button').click()\n",
    "\n",
    "time.sleep(60)\n",
    "driver.find_element_by_xpath('/html/body/div[9]/div/div/div/div[2]/form/div/a').click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data in Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(port = 12345)\n",
    "reviews_db = client.reviewsdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = db.collection_names()\n",
    "print(collection + ' contains: ' + db.collection.count() + ' different strains.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our list of links, this cell actually does all of the scraping\n",
    "\n",
    "by calling our earlier function. This whole process is very straight forward,\n",
    "\n",
    "but it takes quite a bit of time.\n",
    "\n",
    "Note that we have '13' in strain scrape. This is beacuse each page has 8 reviews,\n",
    "\n",
    "and due to time constraints I decided to grab the 100 most recent reviews for \n",
    "\n",
    "as many strains as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in link_list:\n",
    "    link_drop = []\n",
    "    try:\n",
    "        strain_reviews = strain_scrape(link, 13) # 13*8=104, some pages fail to load so it ranges\n",
    "        link_drop.append(strain_reviews)\n",
    "    except:\n",
    "        pass\n",
    "    db.collection.insert_one(link_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm including this hacky bit because it was the only way for me to scrape\n",
    "\n",
    "at one point.\n",
    "\n",
    "First, the initialized list is moved into the function, so it gets cleared \n",
    "\n",
    "by each new strain. Pickle is in there too, so that each strain gets independently\n",
    "\n",
    "appended into local storage (that way we can re-call it and throw it into Mongo).\n",
    "\n",
    "Every project at Metis has had a moment like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in link_list[218:]:\n",
    "    strain_list = []\n",
    "    try:\n",
    "        strain_reviews = strain_scrape(link, 13)\n",
    "        strain_list.append(strain_reviews)\n",
    "    except:\n",
    "        pass\n",
    "    check = pd.read_pickle('check_reviews.pkl')\n",
    "    check = check + strain_list\n",
    "    pickle_out = open('check_reviews.pkl', 'wb')\n",
    "    pickle.dump(check, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j=0\n",
    "for i, c in enumerate(check):\n",
    "    if len(c) == 0:\n",
    "        del(check[i])\n",
    "        del(link_list[i])\n",
    "#         j+=1\n",
    "# j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(check), len(link_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open('link_list.pkl', 'wb')\n",
    "pickle.dump(link_list, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
