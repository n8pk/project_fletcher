{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "import string\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the MVP, files stored in '/mvp_reviews'\n",
    "\n",
    "48 strains x 104 reviews (Leafly is into multiples of 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_reviews = pd.read_pickle('/home/nate/ds/metis/class_work/projects/project_fletcher/pkl/mvp_reviews.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unpack and check raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the way I've stored the reviews is in one large list of lists\n",
    "\n",
    "we'll unpack this, but first a quick peek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stored_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's just look at the first review of the first strain...\n",
    "\n",
    "we're concerned with formatting, thankfully everything is already text because of the way we scraped it\n",
    "\n",
    "however, we still have: numbers, punctuation, capitalization, interesting vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wow! what a strain! very nice body and head high. would not recommend for getting stuff done as everything moved at 10 frames per second... just stay home and enjoy this amazing strain'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stored_reviews[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore a couple different ways to handle cleaning\n",
    "\n",
    "remove = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "\n",
    "clean_list = []\n",
    "for inner in stored_reviews:\n",
    "    strain = ''\n",
    "    \n",
    "    for review in inner:\n",
    "        review = review.translate(remove) # translate runs on C, so it's fast, really fast\n",
    "        # join is another quick function, but not quite as efficient, here's an interpretable implementation using a list comprehension\n",
    "        review = ''.join([i for i in review if not i.isdigit()]) \n",
    "        strain += review.lower()\n",
    "        \n",
    "    clean_list.append(strain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we have text that's ready to see some stronger stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wow what a strain very nice body and head high would not recommend for getting stuff done as everyth'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_list[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exploring spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy is excellent! after spending some time scouring the docs, I came across quite a few practical examples\n",
    "\n",
    "first let's get all of the documents into a format we can clean and lemmatize\n",
    "\n",
    "lemmatization is the process of turning words into their root, so ['describe', 'described', 'describes'] all become 'describe'\n",
    "\n",
    "this first cell where we call nlp is the bulk of the work\n",
    "\n",
    "cleaning was fast, but we need something we can throw into a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(token):\n",
    "    \n",
    "    '''\n",
    "    for each token, determine if it's noise\n",
    "    \n",
    "    i.e. if it's a stopword, too short, or punctuation\n",
    "    '''\n",
    "    \n",
    "    noise = False\n",
    "    if token.is_stop == True:\n",
    "        noise = True\n",
    "    elif token.is_punct == True:\n",
    "        noise = True\n",
    "    elif token.is_digit == True:\n",
    "        noise = True\n",
    "    elif token.is_space == True:\n",
    "        noise = True\n",
    "    elif len(token) < 3:\n",
    "        noise = True\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_reviews = []\n",
    "\n",
    "for strain in clean_list:\n",
    "    tokenized_strain = nlp(strain)\n",
    "    strain_review = ''\n",
    "    for token in tokenized_strain:\n",
    "        if noise(token):\n",
    "            pass\n",
    "        else:\n",
    "            strain_review += str(token.lemma_) + ' '\n",
    "            \n",
    "    tokenized_reviews.append(strain_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wow strain nice body head high recommend get stuff move frame second stay home enjoy amazing strainh'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_reviews[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our data is almost ready for some machine learning!\n",
    "\n",
    "now we need to split every word into it's own place\n",
    "\n",
    "the result is one massive list of lists (again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = []\n",
    "\n",
    "for review in tokenized_reviews:\n",
    "    \n",
    "    preprocessed.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow',\n",
       " 'strain',\n",
       " 'nice',\n",
       " 'body',\n",
       " 'head',\n",
       " 'high',\n",
       " 'recommend',\n",
       " 'get',\n",
       " 'stuff',\n",
       " 'move',\n",
       " 'frame',\n",
       " 'second',\n",
       " 'stay',\n",
       " 'home',\n",
       " 'enjoy',\n",
       " 'amazing',\n",
       " 'strainhappy',\n",
       " 'uplift',\n",
       " 'flower',\n",
       " 'smoke']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_out = open('preprocessed.pkl', 'wb')\n",
    "pickle.dump(tokenized_reviews, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
